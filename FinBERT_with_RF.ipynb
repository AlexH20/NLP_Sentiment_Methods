{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinBERT_with_RF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyNHfzVlcC5rtmg2FIKD5ftx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexH20/Supervised-ML-sentiment-measures/blob/main/FinBERT_with_RF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch\n",
        "!pip3 install transformers -i https://pypi.python.org/simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yK1mwWMfAT5",
        "outputId": "fd1a768f-6c25-4392-e0e5-deca27e9d4cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Looking in indexes: https://pypi.python.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UcGCqHNwcvkX",
        "outputId": "58ea6aac-c3f8-4c38-c5e4-71fd2c1bc06f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:98: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Date Ticker  Nasdaq  Turnover          Size       BTM  \\\n",
            "0      2015-01-02   AAPL       1  1.336802  6.370024e+08  0.187370   \n",
            "1      2015-01-05   AAPL       1  1.344416  6.190077e+08  0.192817   \n",
            "2      2015-01-06   AAPL       1  1.347419  6.190077e+08  0.192817   \n",
            "7      2015-01-13   AAPL       1  1.376746  6.363537e+08  0.187561   \n",
            "8      2015-01-14   AAPL       1  1.384635  6.420037e+08  0.185910   \n",
            "...           ...    ...     ...       ...           ...       ...   \n",
            "119559 2015-03-16    GLW       0  1.066449  3.014349e+07  0.623285   \n",
            "120809 2015-03-04    ADP       1  0.631143  4.214598e+07  0.114092   \n",
            "120824 2015-03-25    ADP       1  0.662422  4.132159e+07  0.116368   \n",
            "124599 2015-03-26    ACN       0  0.724509  5.540726e+07  0.110703   \n",
            "124601 2015-03-30    ACN       0  0.724411  5.930794e+07  0.103422   \n",
            "\n",
            "        pref_alpha       CAR  \\\n",
            "0         0.001312 -0.018882   \n",
            "1         0.001142 -0.000292   \n",
            "2         0.000912  0.012774   \n",
            "7         0.000836  0.012517   \n",
            "8         0.000921 -0.016473   \n",
            "...            ...       ...   \n",
            "119559    0.000265  0.006089   \n",
            "120809    0.001006 -0.019826   \n",
            "120824    0.000919 -0.012489   \n",
            "124599    0.000401  0.064717   \n",
            "124601    0.000304 -0.006981   \n",
            "\n",
            "                                                     Text  word_count  \\\n",
            "0       while the holidays in general and christmas in...        2129   \n",
            "1       apple  has been a darling of the market all ye...        5041   \n",
            "2       apple watch launches within a matter of months...         732   \n",
            "7       apple    is the most valuable company in the w...        1464   \n",
            "8       source apple. india may not be the first count...        1679   \n",
            "...                                                   ...         ...   \n",
            "119559  consistently one of the more popular stocks pe...         292   \n",
            "120809  the adp adp private sector jobs report was rel...          70   \n",
            "120824  computer sciences corporation  csc   will begi...         253   \n",
            "124599  accenture  acn   is due with its 00NUMBER00 re...        2876   \n",
            "124601  00NUMBER00 earnings  last week and the crowd w...         654   \n",
            "\n",
            "              AR  Year  Month  ordered_month  \n",
            "0      -0.009268  2015      1              1  \n",
            "1      -0.009879  2015      1              1  \n",
            "2       0.009766  2015      1              1  \n",
            "7       0.011146  2015      1              1  \n",
            "8       0.001417  2015      1              1  \n",
            "...          ...   ...    ...            ...  \n",
            "119559  0.019097  2015      3              3  \n",
            "120809 -0.020174  2015      3              3  \n",
            "120824 -0.010883  2015      3              3  \n",
            "124599  0.069788  2015      3              3  \n",
            "124601 -0.006418  2015      3              3  \n",
            "\n",
            "[925 rows x 14 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 116/116 [00:10<00:00, 10.60it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0         1         2         3         4         5         6    \\\n",
            "0   0.999113  0.146889 -0.991251  0.709346 -0.780556  0.203437  0.254314   \n",
            "1   0.999486  0.306045 -0.978605  0.832530 -0.764886  0.224046  0.180493   \n",
            "2   0.999542  0.039582 -0.967300  0.714708 -0.884678  0.266150  0.614709   \n",
            "3   0.999250  0.179671 -0.994733  0.582414 -0.454200  0.369296 -0.845250   \n",
            "4   0.999688  0.176820 -0.988194  0.930342 -0.902581  0.188742  0.297039   \n",
            "..       ...       ...       ...       ...       ...       ...       ...   \n",
            "0   0.998588  0.098317 -0.946693  0.290088 -0.663621  0.296852  0.189857   \n",
            "1   0.999568  0.091808 -0.978617  0.952216 -0.937682  0.233454  0.484442   \n",
            "2   0.993600  0.101305 -0.998607  0.655180 -0.257875  0.201596 -0.779295   \n",
            "3   0.999332  0.210643 -0.944445  0.733916 -0.677465  0.434572 -0.200616   \n",
            "4   0.999270  0.068906 -0.995208  0.844726 -0.470104  0.405952 -0.277172   \n",
            "\n",
            "         7         8         9    ...       758       759       760       761  \\\n",
            "0  -0.032235  0.844211  0.237219  ...  0.518357 -0.293386 -0.562190  0.018661   \n",
            "1  -0.093418  0.500693  0.060871  ...  0.302973 -0.334789 -0.101728 -0.263164   \n",
            "2  -0.233585  0.694581  0.236812  ...  0.684515 -0.210250 -0.622956 -0.070143   \n",
            "3  -0.227019  0.916658  0.034276  ...  0.434347 -0.235262 -0.650977 -0.142913   \n",
            "4  -0.026182  0.925542  0.096892  ...  0.520937 -0.294825 -0.314319  0.130985   \n",
            "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
            "0  -0.031330  0.734027  0.186472  ... -0.128678 -0.299668 -0.724826 -0.096191   \n",
            "1   0.123266  0.862678  0.133818  ...  0.517464 -0.312977  0.004883  0.028222   \n",
            "2  -0.012775  0.979642  0.163743  ...  0.708638 -0.093684 -0.846105 -0.000014   \n",
            "3  -0.044454  0.802762  0.115965  ... -0.173529 -0.300283 -0.003180  0.082049   \n",
            "4   0.089231  0.895870  0.050611  ...  0.441497 -0.176939 -0.478108 -0.120983   \n",
            "\n",
            "         762       763       764       765       766       767  \n",
            "0  -0.174496 -0.411402 -0.747098 -0.195054  0.508794 -0.227606  \n",
            "1  -0.118555 -0.121358 -0.899477  0.002065 -0.459064 -0.094098  \n",
            "2   0.128663 -0.066934 -0.903584  0.040723  0.446252  0.016773  \n",
            "3  -0.236788 -0.127198 -0.832032  0.327762  0.620516 -0.184191  \n",
            "4  -0.010230 -0.464444 -0.341150 -0.582433  0.217135 -0.277206  \n",
            "..       ...       ...       ...       ...       ...       ...  \n",
            "0  -0.259272 -0.306843 -0.948489  0.315885 -0.011408 -0.059809  \n",
            "1  -0.228531 -0.484416 -0.785898 -0.691874  0.017785 -0.317178  \n",
            "2   0.023573 -0.414001 -0.738581  0.144558  0.854317 -0.245180  \n",
            "3  -0.388657 -0.280837 -0.921718  0.237304  0.479277 -0.136956  \n",
            "4  -0.128021 -0.475498 -0.759485 -0.142766  0.220537 -0.168960  \n",
            "\n",
            "[925 rows x 768 columns]\n",
            "             Date Ticker  Nasdaq  Turnover          Size       BTM  \\\n",
            "20     2015-02-02   AAPL       1  1.487614  6.909898e+08  0.172730   \n",
            "21     2015-02-03   AAPL       1  1.512839  6.909898e+08  0.172730   \n",
            "22     2015-02-04   AAPL       1  1.525196  6.911064e+08  0.172701   \n",
            "23     2015-02-05   AAPL       1  1.537634  6.964069e+08  0.171387   \n",
            "24     2015-02-06   AAPL       1  1.535326  6.986203e+08  0.170844   \n",
            "...           ...    ...     ...       ...           ...       ...   \n",
            "123358 2015-04-21   EBAY       1  1.445607  6.811507e+07  0.096543   \n",
            "123359 2015-04-22   EBAY       1  1.442887  6.853418e+07  0.095952   \n",
            "123360 2015-04-23   EBAY       1  1.440043  6.894115e+07  0.095386   \n",
            "125869 2015-04-14    PEP       1  0.545459  1.416095e+08  0.084196   \n",
            "125876 2015-04-23    PEP       1  0.541915  1.435902e+08  0.083035   \n",
            "\n",
            "        pref_alpha       CAR  \\\n",
            "20        0.000675 -0.014740   \n",
            "21        0.000971 -0.002690   \n",
            "22        0.001033  0.007809   \n",
            "23        0.001020 -0.009263   \n",
            "24        0.000961  0.005828   \n",
            "...            ...       ...   \n",
            "123358   -0.000071  0.008994   \n",
            "123359   -0.000016  0.036152   \n",
            "123360   -0.000014  0.038449   \n",
            "125869    0.000200  0.003431   \n",
            "125876    0.000060 -0.026413   \n",
            "\n",
            "                                                     Text  word_count  \\\n",
            "20      last week i mused on whether apple  investors ...        1615   \n",
            "21      u. s. capitol building. source flickr user wal...        1700   \n",
            "22      apple  s first fiscal quarter of 00NUMBER00 co...        1586   \n",
            "23      nfl hall of famer steve young spoke at linkedi...        1322   \n",
            "24      the linkedin team at the new york stock exchan...        1123   \n",
            "...                                                   ...         ...   \n",
            "123358  ebay  ebay   is set to post its 00NUMBER00 res...        1791   \n",
            "123359  expected earnings release 00NUMBER00 after hou...         266   \n",
            "123360  let the bidding begin ebay  investors. the onl...         862   \n",
            "125869  top consumer shareswmt flatmcd 00NUMBER00 flat...         266   \n",
            "125876  pepsico inc. pep is the largest food and bever...        1240   \n",
            "\n",
            "              AR  Year  Month  ordered_month  AR_dummy  \n",
            "20      0.000312  2015      2              2       NaN  \n",
            "21     -0.014870  2015      2              2       NaN  \n",
            "22      0.012113  2015      2              2       NaN  \n",
            "23     -0.004410  2015      2              2       NaN  \n",
            "24     -0.004835  2015      2              2       NaN  \n",
            "...          ...   ...    ...            ...       ...  \n",
            "123358  0.007163  2015      4              4       1.0  \n",
            "123359  0.001790  2015      4              4       1.0  \n",
            "123360  0.034153  2015      4              4       1.0  \n",
            "125869  0.008164  2015      4              4       1.0  \n",
            "125876 -0.019489  2015      4              4       0.0  \n",
            "\n",
            "[975 rows x 15 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            " 28%|██▊       | 34/122 [00:03<00:08, 10.26it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-16c1506bfe7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mdata_train_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \"\"\"\n",
            "\u001b[0;32m<ipython-input-15-16c1506bfe7f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-16c1506bfe7f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_id, mask)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpooled_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = df['AR_dummy'].tolist()\n",
        "        self.texts = [tokenizer(text, \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in df['Text']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.1):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('yiyanghkust/finbert-pretrain')\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "\n",
        "        return pooled_output.tolist()\n",
        "\n",
        "def train(model, train_data):\n",
        "\n",
        "    train = Dataset(train_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=8, shuffle=True)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "        train_label = train_label.to(device)\n",
        "        mask = train_input['attention_mask'].to(device)\n",
        "        input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "        df = pd.DataFrame(model(input_id, mask))\n",
        "\n",
        "        frames.append(df)\n",
        "\n",
        "    df_final = pd.concat(frames)\n",
        "\n",
        "    return df_final\n",
        "\n",
        "\n",
        "def split_months(dt):\n",
        "    return [dt[dt[\"ordered_month\"] == y] for y in dt[\"ordered_month\"].unique()]\n",
        "\n",
        "data = pd.read_csv(\"gdrive/My Drive/Thesis/processed data/CAR_regression/datasets_final/data_whole_woScAR.csv\", index_col = False)\n",
        "\n",
        "data_onlytext = data[data[\"word_count\"] != 0]\n",
        "data_onlytext[\"Date\"] = pd.to_datetime(data_onlytext[\"Date\"])\n",
        "data_onlytext[\"Year\"] = [x.year for x in data_onlytext[\"Date\"]]\n",
        "data_onlytext[\"Month\"] = [x.month for x in data_onlytext[\"Date\"]]\n",
        "data_onlytext[\"ordered_month\"] = [((x[1][\"Year\"]-2015)*12 + x[1][\"Month\"]) for x in data_onlytext.iterrows()]\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\n",
        "  \n",
        "data_splt_months = split_months(data_onlytext)\n",
        "\n",
        "i = -1\n",
        "\n",
        "np.random.seed(9000)\n",
        "for _, month in enumerate(data_splt_months):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        data_train = pd.concat([data_splt_months[i], data_splt_months[i+1], data_splt_months[i+2]])\n",
        "        print(data_train)\n",
        "        data_test = data_splt_months[i+3]\n",
        "\n",
        "        model = BertClassifier()\n",
        "        X_train_hidden = train(model, data_train)\n",
        "        X_test_hidden = train(model, data_test)\n",
        "\n",
        "        y_train = data_train[\"AR\"]\n",
        "\n",
        "        \n",
        "        \n",
        "\"\"\"\n",
        "        if i+1 < len(data_splt_months):\n",
        "\n",
        "            data_splt_months[i+3][\"AR_BERT\"] = pred\n",
        "\n",
        "            with open(\"gdrive/My Drive/Thesis/processed data/CAR_regression/BERT_sentiment_dummy_R/\" + str(i+1) + \".csv\", \"w\") as csv_file:\n",
        "                  \n",
        "                  writer = csv.writer(csv_file)\n",
        "                  writer.writerow(\n",
        "                      [\"Date\", \"Ticker\", \"Nasdaq\", \"Turnover\", \"Size\", \"BTM\", \"pref_alpha\", \"CAR\", \"Text\", \"AR\", \"AR_dummy\", \"AR_BERT\"])\n",
        "                  for index, row in data_splt_months[i+3].iterrows():\n",
        "                      writer.writerow([row[\"Date\"], row[\"Ticker\"], row[\"Nasdaq\"], row[\"Turnover\"], row[\"Size\"], row[\"BTM\"], row[\"pref_alpha\"], row[\"CAR\"], row[\"Text\"], row[\"AR\"], row[\"AR_dummy\"], row[\"AR_BERT\"]])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}