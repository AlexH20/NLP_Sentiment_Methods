{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinBERT_with_RF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyOqqd80lqrbZ6gUjlTm+LEr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexH20/Supervised-ML-sentiment-measures/blob/main/FinBERT_with_RF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch\n",
        "!pip3 install transformers -i https://pypi.python.org/simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yK1mwWMfAT5",
        "outputId": "fd1a768f-6c25-4392-e0e5-deca27e9d4cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Looking in indexes: https://pypi.python.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcGCqHNwcvkX",
        "outputId": "4a37f08b-cff7-4c8a-dab1-a455c7ffd0ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:98: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Date Ticker  Nasdaq  Turnover          Size       BTM  \\\n",
            "0      2015-01-02   AAPL       1  1.336802  6.370024e+08  0.187370   \n",
            "1      2015-01-05   AAPL       1  1.344416  6.190077e+08  0.192817   \n",
            "2      2015-01-06   AAPL       1  1.347419  6.190077e+08  0.192817   \n",
            "7      2015-01-13   AAPL       1  1.376746  6.363537e+08  0.187561   \n",
            "8      2015-01-14   AAPL       1  1.384635  6.420037e+08  0.185910   \n",
            "...           ...    ...     ...       ...           ...       ...   \n",
            "119559 2015-03-16    GLW       0  1.066449  3.014349e+07  0.623285   \n",
            "120809 2015-03-04    ADP       1  0.631143  4.214598e+07  0.114092   \n",
            "120824 2015-03-25    ADP       1  0.662422  4.132159e+07  0.116368   \n",
            "124599 2015-03-26    ACN       0  0.724509  5.540726e+07  0.110703   \n",
            "124601 2015-03-30    ACN       0  0.724411  5.930794e+07  0.103422   \n",
            "\n",
            "        pref_alpha       CAR  \\\n",
            "0         0.001312 -0.018882   \n",
            "1         0.001142 -0.000292   \n",
            "2         0.000912  0.012774   \n",
            "7         0.000836  0.012517   \n",
            "8         0.000921 -0.016473   \n",
            "...            ...       ...   \n",
            "119559    0.000265  0.006089   \n",
            "120809    0.001006 -0.019826   \n",
            "120824    0.000919 -0.012489   \n",
            "124599    0.000401  0.064717   \n",
            "124601    0.000304 -0.006981   \n",
            "\n",
            "                                                     Text  word_count  \\\n",
            "0       while the holidays in general and christmas in...        2129   \n",
            "1       apple  has been a darling of the market all ye...        5041   \n",
            "2       apple watch launches within a matter of months...         732   \n",
            "7       apple  Â  is the most valuable company in the w...        1464   \n",
            "8       source apple. india may not be the first count...        1679   \n",
            "...                                                   ...         ...   \n",
            "119559  consistently one of the more popular stocks pe...         292   \n",
            "120809  the adp adp private sector jobs report was rel...          70   \n",
            "120824  computer sciences corporation  csc   will begi...         253   \n",
            "124599  accenture  acn   is due with its 00NUMBER00 re...        2876   \n",
            "124601  00NUMBER00 earnings  last week and the crowd w...         654   \n",
            "\n",
            "              AR  Year  Month  ordered_month  \n",
            "0      -0.009268  2015      1              1  \n",
            "1      -0.009879  2015      1              1  \n",
            "2       0.009766  2015      1              1  \n",
            "7       0.011146  2015      1              1  \n",
            "8       0.001417  2015      1              1  \n",
            "...          ...   ...    ...            ...  \n",
            "119559  0.019097  2015      3              3  \n",
            "120809 -0.020174  2015      3              3  \n",
            "120824 -0.010883  2015      3              3  \n",
            "124599  0.069788  2015      3              3  \n",
            "124601 -0.006418  2015      3              3  \n",
            "\n",
            "[925 rows x 14 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at yiyanghkust/finbert-pretrain were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|ââââââââââ| 116/116 [00:11<00:00, 10.54it/s]\n",
            "100%|ââââââââââ| 48/48 [00:04<00:00, 10.66it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = df['AR'].tolist()\n",
        "        self.texts = [tokenizer(text, \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in df['Text']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.1):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('yiyanghkust/finbert-pretrain')\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "\n",
        "        return pooled_output.tolist()\n",
        "\n",
        "def train(model, train_data):\n",
        "\n",
        "    train = Dataset(train_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=8, shuffle=True)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "        train_label = train_label.to(device)\n",
        "        mask = train_input['attention_mask'].to(device)\n",
        "        input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "        df = pd.DataFrame(model(input_id, mask))\n",
        "\n",
        "        frames.append(df)\n",
        "\n",
        "    df_final = pd.concat(frames)\n",
        "\n",
        "    return df_final\n",
        "\n",
        "\n",
        "def split_months(dt):\n",
        "    return [dt[dt[\"ordered_month\"] == y] for y in dt[\"ordered_month\"].unique()]\n",
        "\n",
        "data = pd.read_csv(\"gdrive/My Drive/Thesis/processed data/CAR_regression/datasets_final/data_whole_woScAR.csv\", index_col = False)\n",
        "\n",
        "data_onlytext = data[data[\"word_count\"] != 0]\n",
        "data_onlytext[\"Date\"] = pd.to_datetime(data_onlytext[\"Date\"])\n",
        "data_onlytext[\"Year\"] = [x.year for x in data_onlytext[\"Date\"]]\n",
        "data_onlytext[\"Month\"] = [x.month for x in data_onlytext[\"Date\"]]\n",
        "data_onlytext[\"ordered_month\"] = [((x[1][\"Year\"]-2015)*12 + x[1][\"Month\"]) for x in data_onlytext.iterrows()]\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\n",
        "  \n",
        "data_splt_months = split_months(data_onlytext)\n",
        "\n",
        "i = -1\n",
        "\n",
        "np.random.seed(9000)\n",
        "for _, month in enumerate(data_splt_months):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        y_predict = []\n",
        "\n",
        "        data_train = pd.concat([data_splt_months[i], data_splt_months[i+1], data_splt_months[i+2]])\n",
        "        print(data_train)\n",
        "        data_test = data_splt_months[i+3]\n",
        "\n",
        "        model = BertClassifier()\n",
        "        X_train_hidden = train(model, data_train)\n",
        "        X_test_hidden = train(model, data_test)\n",
        "\n",
        "        y_train = data_train[\"AR\"]\n",
        "\n",
        "        #Random Forest\n",
        "        rf = RandomForestRegressor(n_estimators=1000)\n",
        "        rf = rf.fit(X_train_hidden, y_train)\n",
        "        y_predict.append(rf.predict(X_test_hidden).tolist())\n",
        "\n",
        "        print(y_predict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\"\"\"\n",
        "\n",
        "        data_splt_months[i+3][\"AR_BERT\"] = pred\n",
        "\n",
        "        with open(\"gdrive/My Drive/Thesis/processed data/CAR_regression/BERT_sentiment_dummy_R/\" + str(i+1) + \".csv\", \"w\") as csv_file:\n",
        "              \n",
        "              writer = csv.writer(csv_file)\n",
        "              writer.writerow(\n",
        "                  [\"Date\", \"Ticker\", \"Nasdaq\", \"Turnover\", \"Size\", \"BTM\", \"pref_alpha\", \"CAR\", \"Text\", \"AR\", \"AR_dummy\", \"AR_BERT\"])\n",
        "              for index, row in data_splt_months[i+3].iterrows():\n",
        "                  writer.writerow([row[\"Date\"], row[\"Ticker\"], row[\"Nasdaq\"], row[\"Turnover\"], row[\"Size\"], row[\"BTM\"], row[\"pref_alpha\"], row[\"CAR\"], row[\"Text\"], row[\"AR\"], row[\"AR_dummy\"], row[\"AR_BERT\"]])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}