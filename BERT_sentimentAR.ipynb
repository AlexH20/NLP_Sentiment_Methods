{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_sentimentAR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO8Pri0QnlxDm5CJp9FvfcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexH20/Supervised-ML-sentiment-measures/blob/main/BERT_sentimentAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch\n",
        "!pip3 install transformers -i https://pypi.python.org/simple\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yK1mwWMfAT5",
        "outputId": "0f3dbacf-2269-4e33-c522-fced43de9f99"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Looking in indexes: https://pypi.python.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 81.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcGCqHNwcvkX",
        "outputId": "2ec5f0ec-fed6-4c9e-d5e1-2bd93a158e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "                       Date Ticker  Nasdaq  Turnover          Size       BTM  \\\n",
            "0       2015-01-02 00:00:00   AAPL       1  1.336802  6.370024e+08  0.187370   \n",
            "1       2015-01-05 00:00:00   AAPL       1  1.344416  6.190077e+08  0.192817   \n",
            "2       2015-01-06 00:00:00   AAPL       1  1.347419  6.190077e+08  0.192817   \n",
            "3       2015-01-07 00:00:00   AAPL       1  1.345351  6.190660e+08  0.192799   \n",
            "4       2015-01-08 00:00:00   AAPL       1  1.351682  6.277467e+08  0.190132   \n",
            "...                     ...    ...     ...       ...           ...       ...   \n",
            "127053  2019-12-24 00:00:00    PEP       1  0.496903  1.910934e+08  0.077376   \n",
            "127054  2019-12-26 00:00:00    PEP       1  0.498080  1.906053e+08  0.077574   \n",
            "127055  2019-12-27 00:00:00    PEP       1  0.494766  1.906053e+08  0.077574   \n",
            "127056  2019-12-30 00:00:00    PEP       1  0.497787  1.903027e+08  0.077697   \n",
            "127057  2019-12-31 00:00:00    PEP       1  0.495126  1.903027e+08  0.077697   \n",
            "\n",
            "        pref_alpha       CAR  \\\n",
            "0         0.001312 -0.018882   \n",
            "1         0.001142 -0.000292   \n",
            "2         0.000912  0.012774   \n",
            "3         0.000963  0.024813   \n",
            "4         0.000958  0.030356   \n",
            "...            ...       ...   \n",
            "127053    0.000099 -0.007551   \n",
            "127054   -0.000036  0.000779   \n",
            "127055   -0.000018  0.006623   \n",
            "127056   -0.000052 -0.004169   \n",
            "127057    0.000004 -0.017633   \n",
            "\n",
            "                                                     Text  word_count  \\\n",
            "0       while the holidays in general and christmas in...        2129   \n",
            "1       apple  has been a darling of the market all ye...        5041   \n",
            "2       apple watch launches within a matter of months...         732   \n",
            "3                                                                   0   \n",
            "4                                                                   0   \n",
            "...                                                   ...         ...   \n",
            "127053                                                              0   \n",
            "127054                                                              0   \n",
            "127055                                                              0   \n",
            "127056                                                              0   \n",
            "127057                                                              0   \n",
            "\n",
            "              AR  \n",
            "0      -0.009268  \n",
            "1      -0.009879  \n",
            "2       0.009766  \n",
            "3       0.002899  \n",
            "4       0.021562  \n",
            "...          ...  \n",
            "127053 -0.001474  \n",
            "127054 -0.006077  \n",
            "127055  0.006861  \n",
            "127056 -0.000203  \n",
            "127057 -0.003986  \n",
            "\n",
            "[127058 rows x 11 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:152: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:153: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:154: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:155: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Date Ticker  Nasdaq  Turnover          Size       BTM  \\\n",
            "0      2015-01-02   AAPL       1  1.336802  6.370024e+08  0.187370   \n",
            "1      2015-01-05   AAPL       1  1.344416  6.190077e+08  0.192817   \n",
            "2      2015-01-06   AAPL       1  1.347419  6.190077e+08  0.192817   \n",
            "7      2015-01-13   AAPL       1  1.376746  6.363537e+08  0.187561   \n",
            "8      2015-01-14   AAPL       1  1.384635  6.420037e+08  0.185910   \n",
            "...           ...    ...     ...       ...           ...       ...   \n",
            "119559 2015-03-16    GLW       0  1.066449  3.014349e+07  0.623285   \n",
            "120809 2015-03-04    ADP       1  0.631143  4.214598e+07  0.114092   \n",
            "120824 2015-03-25    ADP       1  0.662422  4.132159e+07  0.116368   \n",
            "124599 2015-03-26    ACN       0  0.724509  5.540726e+07  0.110703   \n",
            "124601 2015-03-30    ACN       0  0.724411  5.930794e+07  0.103422   \n",
            "\n",
            "        pref_alpha       CAR  \\\n",
            "0         0.001312 -0.018882   \n",
            "1         0.001142 -0.000292   \n",
            "2         0.000912  0.012774   \n",
            "7         0.000836  0.012517   \n",
            "8         0.000921 -0.016473   \n",
            "...            ...       ...   \n",
            "119559    0.000265  0.006089   \n",
            "120809    0.001006 -0.019826   \n",
            "120824    0.000919 -0.012489   \n",
            "124599    0.000401  0.064717   \n",
            "124601    0.000304 -0.006981   \n",
            "\n",
            "                                                     Text  word_count  \\\n",
            "0       while the holidays in general and christmas in...        2129   \n",
            "1       apple  has been a darling of the market all ye...        5041   \n",
            "2       apple watch launches within a matter of months...         732   \n",
            "7       apple    is the most valuable company in the w...        1464   \n",
            "8       source apple. india may not be the first count...        1679   \n",
            "...                                                   ...         ...   \n",
            "119559  consistently one of the more popular stocks pe...         292   \n",
            "120809  the adp adp private sector jobs report was rel...          70   \n",
            "120824  computer sciences corporation  csc   will begi...         253   \n",
            "124599  accenture  acn   is due with its 00NUMBER00 re...        2876   \n",
            "124601  00NUMBER00 earnings  last week and the crowd w...         654   \n",
            "\n",
            "              AR  Year  Month  ordered_month  \n",
            "0      -0.009268  2015      1              1  \n",
            "1      -0.009879  2015      1              1  \n",
            "2       0.009766  2015      1              1  \n",
            "7       0.011146  2015      1              1  \n",
            "8       0.001417  2015      1              1  \n",
            "...          ...   ...    ...            ...  \n",
            "119559  0.019097  2015      3              3  \n",
            "120809 -0.020174  2015      3              3  \n",
            "120824 -0.010883  2015      3              3  \n",
            "124599  0.069788  2015      3              3  \n",
            "124601 -0.006418  2015      3              3  \n",
            "\n",
            "[925 rows x 14 columns]\n",
            "             Date Ticker  Nasdaq  Turnover          Size       BTM  \\\n",
            "61     2015-04-01   AAPL       1  1.639625  7.247734e+08  0.164679   \n",
            "62     2015-04-02   AAPL       1  1.630394  7.237249e+08  0.164918   \n",
            "63     2015-04-06   AAPL       1  1.630430  7.417816e+08  0.160903   \n",
            "64     2015-04-07   AAPL       1  1.627906  7.417816e+08  0.160903   \n",
            "65     2015-04-08   AAPL       1  1.625957  7.339765e+08  0.162614   \n",
            "...           ...    ...     ...       ...           ...       ...   \n",
            "123358 2015-04-21   EBAY       1  1.445607  6.811507e+07  0.096543   \n",
            "123359 2015-04-22   EBAY       1  1.442887  6.853418e+07  0.095952   \n",
            "123360 2015-04-23   EBAY       1  1.440043  6.894115e+07  0.095386   \n",
            "125869 2015-04-14    PEP       1  0.545459  1.416095e+08  0.084196   \n",
            "125876 2015-04-23    PEP       1  0.541915  1.435902e+08  0.083035   \n",
            "\n",
            "        pref_alpha       CAR  \\\n",
            "61        0.001015  0.005836   \n",
            "62        0.000867  0.014333   \n",
            "63        0.000843  0.001074   \n",
            "64        0.000777 -0.015184   \n",
            "65        0.000850 -0.002603   \n",
            "...            ...       ...   \n",
            "123358   -0.000071  0.008994   \n",
            "123359   -0.000016  0.036152   \n",
            "123360   -0.000014  0.038449   \n",
            "125869    0.000200  0.003431   \n",
            "125876    0.000060 -0.026413   \n",
            "\n",
            "                                                     Text  word_count  \\\n",
            "61      china is quickly becoming the most important m...        3061   \n",
            "62      image source apple. apple  has announced the d...        2158   \n",
            "63      source apple. a new era is on the horizon for ...        4208   \n",
            "64      iphone 00NUMBER00 and iphone 00NUMBER00 plus. ...        2737   \n",
            "65      as wearable connected technology continues to ...        1578   \n",
            "...                                                   ...         ...   \n",
            "123358  ebay  ebay   is set to post its 00NUMBER00 res...        1791   \n",
            "123359  expected earnings release 00NUMBER00 after hou...         266   \n",
            "123360  let the bidding begin ebay  investors. the onl...         862   \n",
            "125869  top consumer shareswmt flatmcd 00NUMBER00 flat...         266   \n",
            "125876  pepsico inc. pep is the largest food and bever...        1240   \n",
            "\n",
            "              AR  Year  Month  ordered_month  \n",
            "61      0.001234  2015      4              4  \n",
            "62      0.004603  2015      4              4  \n",
            "63      0.009617  2015      4              4  \n",
            "64     -0.008386  2015      4              4  \n",
            "65     -0.006839  2015      4              4  \n",
            "...          ...   ...    ...            ...  \n",
            "123358  0.007163  2015      4              4  \n",
            "123359  0.001790  2015      4              4  \n",
            "123360  0.034153  2015      4              4  \n",
            "125869  0.008164  2015      4              4  \n",
            "125876 -0.019489  2015      4              4  \n",
            "\n",
            "[377 rows x 14 columns]\n",
            "925\n",
            "377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:179: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 116/116 [00:28<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.087                 | Train Accuracy:  0.488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 107/116 [00:26<00:02,  4.08it/s]"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = df['AR_dummy'].tolist()\n",
        "        self.texts = [tokenizer(text, \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in df['Text']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.1):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.bert.trainable = False\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.sigmoid(linear_output)\n",
        "\n",
        "        return final_layer\n",
        "\n",
        "def train(model, train_data, learning_rate, epochs):\n",
        "\n",
        "    train = Dataset(train_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=8, shuffle=True)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(output, train_label)\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f}')\n",
        "            \n",
        "\n",
        "def evaluate(model, test_data):\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    prediction = []\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=8)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "\n",
        "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "              total_acc_test += acc\n",
        "              prediction.append(output.argmax(dim=1).tolist())\n",
        "    \n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "    \n",
        "    return sum(prediction, [])\n",
        "\n",
        "def split_months(dt):\n",
        "    return [dt[dt[\"ordered_month\"] == y] for y in dt[\"ordered_month\"].unique()]\n",
        "\n",
        "data = pd.read_csv(\"gdrive/My Drive/Thesis/processed data/CAR_regression/datasets_final/data_whole_woScAR.csv\", index_col = False)\n",
        "print(data)\n",
        "\n",
        "data_onlytext = data[data[\"word_count\"] != 0]\n",
        "data_onlytext[\"Date\"] = pd.to_datetime(data_onlytext[\"Date\"])\n",
        "data_onlytext[\"Year\"] = [x.year for x in data_onlytext[\"Date\"]]\n",
        "data_onlytext[\"Month\"] = [x.month for x in data_onlytext[\"Date\"]]\n",
        "data_onlytext[\"ordered_month\"] = [((x[1][\"Year\"]-2015)*12 + x[1][\"Month\"]) for x in data_onlytext.iterrows()]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "EPOCHS = 5\n",
        "LR = 1e-5\n",
        "             \n",
        "data_splt_months = split_months(data_onlytext)\n",
        "\n",
        "i = -1\n",
        "\n",
        "np.random.seed(9000)\n",
        "for _, month in enumerate(data_splt_months):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        data_train = pd.concat([data_splt_months[i], data_splt_months[i+1], data_splt_months[i+2]])\n",
        "        print(data_train)\n",
        "        data_test = data_splt_months[i+3]\n",
        "        print(data_test)\n",
        "\n",
        "        print(len(data_train))\n",
        "        print(len(data_test))\n",
        "\n",
        "        data_train[\"AR_dummy\"] = np.where(data_train[\"AR\"] > 0, 1, 0)\n",
        "        data_test[\"AR_dummy\"] = np.where(data_test[\"AR\"] > 0, 1, 0)\n",
        "\n",
        "        model = BertClassifier()\n",
        "        train(model, data_train, LR, EPOCHS)\n",
        "        pred = evaluate(model, data_test)\n",
        "\n",
        "        if i+1 < len(data_splt_months):\n",
        "\n",
        "            data_splt_months[i+3][\"AR_BERT\"] = pred\n",
        "\n",
        "            with open(\"gdrive/My Drive/Thesis/processed data/CAR_regression/BERT_sentiment_dummy_R/\" + str(i+1) + \".csv\", \"w\") as csv_file:\n",
        "                  \n",
        "                  writer = csv.writer(csv_file)\n",
        "                  writer.writerow(\n",
        "                      [\"Date\", \"Ticker\", \"Nasdaq\", \"Turnover\", \"Size\", \"BTM\", \"pref_alpha\", \"CAR\", \"Text\", \"AR\", \"AR_dummy\", \"AR_BERT\"])\n",
        "                  for index, row in data_splt_months[i+3].iterrows():\n",
        "                      writer.writerow([row[\"Date\"], row[\"Ticker\"], row[\"Nasdaq\"], row[\"Turnover\"], row[\"Size\"], row[\"BTM\"], row[\"pref_alpha\"], row[\"CAR\"], row[\"Text\"], row[\"AR\"], row[\"AR_dummy\"], row[\"AR_BERT\"]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}