{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IhjuzCV0LyBp",
        "outputId": "030e3bf6-c609-43db-e42e-a30600a8e698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "          Date Ticker  Nasdaq  Turnover          Size       BTM  pref_alpha  \\\n",
            "0   2015-01-01   AAPL       1  1.336802  6.370024e+08  0.187370    0.001312   \n",
            "1   2015-01-02   AAPL       1  1.336802  6.370024e+08  0.187370    0.001312   \n",
            "2   2015-01-05   AAPL       1  1.344416  6.190077e+08  0.192817    0.001142   \n",
            "3   2015-01-06   AAPL       1  1.347419  6.190077e+08  0.192817    0.000912   \n",
            "4   2015-01-07   AAPL       1  1.345351  6.190660e+08  0.192799    0.000963   \n",
            "5   2015-01-08   AAPL       1  1.351682  6.277467e+08  0.190132    0.000958   \n",
            "6   2015-01-09   AAPL       1  1.350462  6.518661e+08  0.183097    0.000957   \n",
            "7   2015-01-12   AAPL       1  1.371621  6.363537e+08  0.187561    0.000885   \n",
            "8   2015-01-13   AAPL       1  1.376746  6.363537e+08  0.187561    0.000836   \n",
            "9   2015-01-14   AAPL       1  1.384635  6.420037e+08  0.185910    0.000921   \n",
            "10  2015-01-15   AAPL       1  1.392119  6.395573e+08  0.186621    0.000991   \n",
            "11  2015-01-16   AAPL       1  1.390707  6.221996e+08  0.191828    0.000985   \n",
            "12  2015-01-19   AAPL       1  1.416145  6.332666e+08  0.188475    0.000994   \n",
            "13  2015-01-20   AAPL       1  1.416145  6.332666e+08  0.188475    0.000994   \n",
            "14  2015-01-21   AAPL       1  1.425469  6.332666e+08  0.188475    0.000879   \n",
            "15  2015-01-22   AAPL       1  1.437932  6.381012e+08  0.187047    0.000738   \n",
            "16  2015-01-23   AAPL       1  1.436499  6.547017e+08  0.182304    0.000745   \n",
            "17  2015-01-26   AAPL       1  1.443372  6.587790e+08  0.181176    0.000790   \n",
            "18  2015-01-27   AAPL       1  1.449763  6.587790e+08  0.181176    0.000775   \n",
            "19  2015-01-28   AAPL       1  1.457559  6.357130e+08  0.187750    0.000800   \n",
            "\n",
            "         CAR                                               Text  word_count  \n",
            "0  -0.018882  _ _ holiday _ gener _ christma _ particular _ ...         706  \n",
            "1  -0.018882  stock _ continu _ lose altitud friday eras ear...        1423  \n",
            "2  -0.000292  appl  _ _ _ darl _ _ market all year long.shar...        5041  \n",
            "3   0.012774  appl watch launch within _ matter _ month.sour...         732  \n",
            "4   0.024813                                                NaN           0  \n",
            "5   0.030356                                                NaN           0  \n",
            "6  -0.008310                                                NaN           0  \n",
            "7  -0.005940                                                NaN           0  \n",
            "8   0.012517  appl    _ _ most valuabl compani _ _ world _ _...        1464  \n",
            "9  -0.016473  sourc appl.india may not _ _ first countri _ c...        1679  \n",
            "10 -0.039234  _ latest talli _ analyst opinion _ _ major bro...         810  \n",
            "11  0.003676                                                NaN           0  \n",
            "12  0.028780  appl  still hasnt announc _ _ new smartwatch _...        4073  \n",
            "13  0.028780  appl   i _ _ _ rough start _ 00number00._ mani...        1588  \n",
            "14  0.014510  _ most _ 00number00 _ no wrong _ _ eye _ inves...        3804  \n",
            "15  0.020865  _ appl  s fiscal 00number00 first quarter anal...        2265  \n",
            "16  0.006049  investor _ alway look _ stock _ _ pois _ beat ...          64  \n",
            "17 -0.028333  sourc twitter.although _ cant _ _ certain id _...        6375  \n",
            "18  0.043497  last year dure appl  s fiscal first quarter _ ...        6376  \n",
            "19  0.095135  buyout rumor _ sent informatica  stock higher ...        2255  \n",
            "30364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2575\n",
            "630\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-25bd6783e689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m#Sparse matrices as input for ML models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sparsematrix_and_car\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sparsematrix_and_car\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-25bd6783e689>\u001b[0m in \u001b[0;36mget_sparsematrix_and_car\u001b[0;34m(data_train, data_test)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Initialize dictionary within dictionary with keys according to all ngrams found in training dataset. Frankel, Jennings and Lee (2021) (modified for own needs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mwrd_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrd_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import sys\n",
        "from scipy.sparse import csc_matrix, vstack\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "from google.colab import drive \n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "data = pd.read_csv(\"gdrive/My Drive/Thesis/processed data/data_whole_wS.csv\", index_col = False)\n",
        "\n",
        "print(data.head(20))\n",
        "\n",
        "data_onlytext = data.dropna()\n",
        "data_onlytext[\"Date\"] = pd.to_datetime(data_onlytext[\"Date\"])\n",
        "data_onlytext[\"Year\"] = [x.year for x in data_onlytext[\"Date\"]]\n",
        "data_onlytext[\"Month\"] = [x.month for x in data_onlytext[\"Date\"]]\n",
        "data_onlytext[\"ordered_month\"] = [((x[1][\"Year\"]-2015)*12 + x[1][\"Month\"]) for x in data_onlytext.iterrows()]\n",
        "\n",
        "#Create a sparse matrix with the dictionary returned by the function get_ngrams Frankel, Jennings and Lee (2021) (modified for own needs)\n",
        "\n",
        "def sparse_mat(data):\n",
        "    row1 = []\n",
        "    col1 = []\n",
        "    data1 = []\n",
        "\n",
        "    #Iterate through dictionary from get_sparsematrix_and_car to create sparse matrix (value in interation are all ngram counts of observation 'key').\n",
        "\n",
        "    for key, value in data.items():\n",
        "\n",
        "        value_n = list(value.values())\n",
        "\n",
        "        for e, elem in enumerate(value_n):\n",
        "            colnum = e\n",
        "            value = elem\n",
        "\n",
        "            row1.append(key)\n",
        "            col1.append(colnum)\n",
        "            data1.append(value)\n",
        "\n",
        "    X = csc_matrix((data1, (row1, col1)))  # Sparse matrix of rows (observations) and columns (independent variables)\n",
        "\n",
        "    return X\n",
        "\n",
        "#Function to get n_grams count of whole training data set\n",
        "#This function will iterate over all text items in the training data set and return all unique one- and twograms within the training data\n",
        "\n",
        "def get_ngrams(data):\n",
        "\n",
        "    onegrams = []\n",
        "    twograms = []\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "\n",
        "        sentences = row[\"Text\"].split('.')\n",
        "\n",
        "        #### EXTRACT ALL ONE AND TWO WORD PHRASES #### Frankel, Jennings and Lee (2021)\n",
        "\n",
        "        for sentence in sentences:\n",
        "\n",
        "            sentence = sentence.replace('.', '').strip()\n",
        "\n",
        "            allwords = sentence.split(' ')\n",
        "\n",
        "            for w, word in enumerate(allwords):\n",
        "                word0 = allwords[w]\n",
        "                try:\n",
        "                    word1 = allwords[w + 1]\n",
        "                except Exception:\n",
        "                    word1 = ''\n",
        "\n",
        "                if word0.strip() != '.' and word0.strip() != '':\n",
        "                    onegrams.append(word0)\n",
        "\n",
        "                    if word1.strip() != '.' and word1.strip() != '':\n",
        "                        twogram = word0 + ' ' + word1\n",
        "                        twograms.append(twogram)\n",
        "\n",
        "    n_grams_dict = {}\n",
        "\n",
        "    uniqueonegrams = list(set(onegrams))\n",
        "    uniqueonegrams = sorted(uniqueonegrams)\n",
        "\n",
        "    uniquetwograms = list(set(twograms))\n",
        "    uniquetwograms = sorted(uniquetwograms)\n",
        "\n",
        "    ngrams = uniqueonegrams + uniquetwograms\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "#The function get_ngrams_eachtxt_sparsematrix takes the ngram_file with all one- and twograms from the function above\n",
        "#and iterates again over each text item (either training or test data) to count occurence of the words from the ngram dictionary.\n",
        "#This function returns a nested dictionary which represents for each txt item in dataset the training dataset ngram occurences\n",
        "\n",
        "def get_sparsematrix_and_car(data_train, data_test):\n",
        "\n",
        "    ngram_list = get_ngrams(data_train)\n",
        "\n",
        "    wrd_list = ngram_list\n",
        "\n",
        "    wrd_list = sorted(wrd_list)\n",
        "    wrd_list = tuple(wrd_list)\n",
        "\n",
        "    # Initialize dependent variable list (CAR)\n",
        "    car = []\n",
        "\n",
        "    # Initialize dictionary with as many keys as txt items in dataset\n",
        "    wrd_dictionary = dict.fromkeys(range(50))\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "\n",
        "    for index, row in data_test.iterrows():\n",
        "\n",
        "        car.append(row[\"CAR\"])\n",
        "\n",
        "        sentences = row[\"Text\"].split('.')\n",
        "\n",
        "        print(j)\n",
        "\n",
        "        # Initialize dictionary within dictionary with keys according to all ngrams found in training dataset. Frankel, Jennings and Lee (2021) (modified for own needs)\n",
        "        wrd_dictionary[i] = dict.fromkeys(wrd_list, 0)\n",
        "\n",
        "        for sentence in sentences:\n",
        "\n",
        "            sentence = sentence.replace('.', '').strip()\n",
        "            allwords = sentence.split(' ')\n",
        "\n",
        "            for w, word in enumerate(allwords):\n",
        "                word0 = allwords[w]\n",
        "                try:\n",
        "                    word1 = allwords[w + 1]\n",
        "                except Exception:\n",
        "                    word1 = ''\n",
        "\n",
        "                # Add count of found ngrams occurence to dictionary\n",
        "                if word0.strip() != '.' and word0.strip() != '':\n",
        "                    if word0 in wrd_dictionary[i].keys():\n",
        "                        wrd_dictionary[i][word0] = wrd_dictionary[i][word0] + 1\n",
        "\n",
        "                    if word1.strip() != '.' and word1.strip() != '':\n",
        "                        if word0 + ' ' + word1 in wrd_dictionary[i].keys():\n",
        "                            wrd_dictionary[i][word0 + ' ' + word1] = wrd_dictionary[i][word0 + ' ' + word1] + 1\n",
        "\n",
        "        i += 1\n",
        "        j += 1\n",
        "        \n",
        "        #i and j necessary due to RAM overload. i serves as marker to create a sparse matrix every 125 observations.\n",
        "        #j serves as marker to concatenate the sparse matrices and to stop the for loop. In last iteration all keys of the word dictionary need to be deleted with 0 entries for sparse matrix function\n",
        "\n",
        "        if j == len(data_test):\n",
        "            keys_to_remove = (j % 50)\n",
        "            for key in range(keys_to_remove, 50):\n",
        "                del wrd_dictionary[key]\n",
        "            spar_mat_i = sparse_mat(wrd_dictionary)\n",
        "            spar_mat = vstack((spar_mat, spar_mat_i))\n",
        "            break\n",
        "        \n",
        "        if i % 50 == 0:\n",
        "            spar_mat_i = sparse_mat(wrd_dictionary)\n",
        "            if j != 50:\n",
        "              spar_mat = vstack((spar_mat, spar_mat_i))\n",
        "              wrd_dictionary = dict.fromkeys(range(50))\n",
        "              i = 0\n",
        "            else:\n",
        "              spar_mat = spar_mat_i\n",
        "              wrd_dictionary = dict.fromkeys(range(50))\n",
        "              i = 0\n",
        "\n",
        "    return spar_mat, car\n",
        "\n",
        "def split_months(dt):\n",
        "    return [dt[dt[\"ordered_month\"] == y] for y in dt[\"ordered_month\"].unique()]\n",
        "\n",
        "data_splt_months = split_months(data_onlytext)\n",
        "\n",
        "i = 45\n",
        "\n",
        "np.random.seed(9000)\n",
        "for _, month in enumerate(data_splt_months):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        data_train = pd.concat([data_splt_months[i], data_splt_months[i+1], data_splt_months[i+2]])\n",
        "        data_test = data_splt_months[i+3]\n",
        "\n",
        "        print(len(data_train))\n",
        "        print(len(data_test))\n",
        "\n",
        "        y_predict = []\n",
        "\n",
        "        if i+1 < len(data_splt_months):\n",
        "\n",
        "            #Sparse matrices as input for ML models\n",
        "            X_train, y_train = get_sparsematrix_and_car(data_train, data_train)\n",
        "            X_test, y_test = get_sparsematrix_and_car(data_train, data_test)\n",
        "\n",
        "            #Random Forest\n",
        "            rf = RandomForestRegressor(n_estimators=1000, max_features='sqrt')\n",
        "            rf = rf.fit(X_train, y_train)\n",
        "            y_predict.append(rf.predict(X_test))\n",
        "\n",
        "            data_splt_months[i+3][\"CAR_RF\"] = y_predict[0].tolist()\n",
        "\n",
        "            with open(\"gdrive/My Drive/Thesis/processed data/RF_sentiment/\" + str(i+1) + \".csv\", \"w\") as csv_file:\n",
        "                  \n",
        "                  writer = csv.writer(csv_file)\n",
        "                  writer.writerow(\n",
        "                      [\"Date\", \"Ticker\", \"Nasdaq\", \"Turnover\", \"Size\", \"BTM\",\n",
        "                      \"pref_alpha\", \"CAR\", \"Text\", \"CAR_RF\"])\n",
        "                  for index, row in data_splt_months[i+3].iterrows():\n",
        "                      writer.writerow([row[\"Date\"], row[\"Ticker\"], row[\"Nasdaq\"], row[\"Turnover\"], row[\"Size\"], row[\"BTM\"], row[\"pref_alpha\"], row[\"CAR\"], row[\"Text\"], row[\"CAR_RF\"]])\n",
        "            \n",
        "\n",
        "        \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Get_RF_sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}