{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Descriptive statistics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive statistics"
      ],
      "metadata": {
        "id": "eNcHCHjlgCG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code describes the dataset used in the sentiment analysis"
      ],
      "metadata": {
        "id": "NH3uokrRgD97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slKxsYyGa1wQ",
        "outputId": "f6527341-dbfc-4fc2-8454-16eddac6d9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "         Year  Observations Earliest Observation Latest Observation  Mean Word Count  Company Count\n",
            "0        2009           112           2009-06-26         2009-12-31       783.508929             71\n",
            "1        2010          2836           2010-01-01         2010-12-31       408.525035            893\n",
            "2        2011          4331           2011-01-01         2011-12-30       428.202263           1039\n",
            "3        2012         10462           2012-01-03         2012-12-31       311.080960           2379\n",
            "4        2013         13480           2013-01-01         2013-12-31       258.271588           3210\n",
            "5        2014         26715           2014-01-01         2014-12-31       274.373573           4188\n",
            "6        2015         36496           2015-01-01         2015-12-31       317.943062           4973\n",
            "7        2016         37777           2016-01-01         2016-12-31       363.179739           5029\n",
            "8        2017         40828           2017-01-01         2017-12-31       474.660380           5016\n",
            "9        2018         80062           2018-01-01         2018-12-31       662.821826           6318\n",
            "10       2019         67782           2019-01-01         2019-08-28      1199.044909           5610\n",
            "11  All years        320881           2009-06-26         2019-08-28       611.473481           9546\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pysentiment2 as ps\n",
        "from google.colab import drive \n",
        "import numpy as np\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"gdrive/My Drive/Thesis/processed data/data_processed_withoutPS.csv\")\n",
        "\n",
        "#Prepare data for descriptive statistics\n",
        "data[\"Date\"] = pd.to_datetime(data[\"Date\"], format = \"%Y-%m-%d\")\n",
        "data.sort_values(by= \"Date\", inplace = True)\n",
        "data.drop_duplicates(inplace = True)\n",
        "\n",
        "#Function to split initial dataframe into dataframes grouped by year\n",
        "def split_years(dt):\n",
        "    dt[\"Year\"] = dt[\"Date\"].dt.year\n",
        "    return [dt[dt[\"Year\"] == y] for y in dt[\"Year\"].unique()]\n",
        "\n",
        "data_splt_years = split_years(data)\n",
        "data_fill = []\n",
        "\n",
        "for df_year_splt in data_splt_years:\n",
        "    year = df_year_splt[\"Date\"].iloc[0].year\n",
        "    obs_count = len(df_year_splt)\n",
        "    earliest_obs = df_year_splt[\"Date\"].iloc[0]\n",
        "    latest_obs = df_year_splt[\"Date\"].iloc[-1]\n",
        "    mean_word_count = df_year_splt[\"Word_count\"].mean()\n",
        "    company_count = df_year_splt[\"Ticker\"].nunique()\n",
        "\n",
        "    data_fill.append([year, obs_count, earliest_obs, latest_obs, mean_word_count, company_count])\n",
        "\n",
        "#Calculate the metrics for the whole dataset\n",
        "obs_count = len(data)\n",
        "earliest_obs = data[\"Date\"].iloc[0]\n",
        "latest_obs = data[\"Date\"].iloc[-1]\n",
        "mean_word_count = data[\"Word_count\"].mean()\n",
        "company_count = data[\"Ticker\"].nunique()\n",
        "\n",
        "data_fill.append([\"All years\", obs_count, earliest_obs, latest_obs, mean_word_count, company_count])\n",
        "\n",
        "df_by_year = pd.DataFrame(data_fill ,columns = [\"Year\", \"Observations\", \"Earliest Observation\", \"Latest Observation\", \"Mean Word Count\", \"Company Count\"])\n",
        "\n",
        "print(df_by_year)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get an overview of the data with respect to the companies"
      ],
      "metadata": {
        "id": "xxIjEWsrdLII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_data_company =  data.groupby(\"Ticker\").nunique().sort_values(by = \"Text\", ascending = False)\n",
        "df_unique_data_company = pd.DataFrame(unique_data_company, columns = [\"Date\", \"Text\", \"Word_count\", \"Year\"])\n",
        "\n",
        "del df_unique_data_company[\"Word_count\"]\n",
        "df_unique_data_company.rename(columns={\"Text\":\"News Articles\"}, inplace=True)\n",
        "df_unique_data_company[\"Average Articles p.d.\"] = round(df_unique_data_company[\"News Articles\"] / df_unique_data_company[\"Date\"], 2)\n",
        "\n",
        "print(df_unique_data_company.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBT9eKsFdRI0",
        "outputId": "69e646a1-a4f8-4552-e7d1-1aec4bc29257"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Date  News Articles  Year  Average Articles p.d.\n",
            "Ticker                                                  \n",
            "AAPL    1826           7025    10                   3.85\n",
            "AMZN    1009           2857    11                   2.83\n",
            "FB      1008           2156     8                   2.14\n",
            "TSLA    1114           2129     9                   1.91\n",
            "SPX     1161           1909     9                   1.64\n",
            "BA       708           1516    10                   2.14\n",
            "NFLX     726           1197    10                   1.65\n",
            "DIS      828           1177    10                   1.42\n",
            "INTC     803           1114    11                   1.39\n",
            "BAC      741           1101    11                   1.49\n",
            "F        681           1015    10                   1.49\n",
            "EFX      352           1010     8                   2.87\n",
            "GLD      643            982    11                   1.53\n",
            "GE       668            943    10                   1.41\n",
            "DAX      707            934     6                   1.32\n",
            "MSFT     666            883    10                   1.33\n",
            "SBUX     616            829    10                   1.35\n",
            "GM       570            817    10                   1.43\n",
            "IBM      566            738    10                   1.30\n",
            "JPM      555            732    10                   1.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, we check the sentiment indices of the dataset given by the dictionary methods using the Harvard Psychsosocological Dictionary and the Loughran and McDonald Dictionary."
      ],
      "metadata": {
        "id": "2S1gZ0SXDY_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hiv4 = ps.HIV4()\n",
        "lm = ps.LM()\n",
        "\n",
        "data_fill = []\n",
        "\n",
        "for df_year_splt in data_splt_years:\n",
        "\n",
        "    year = df_year_splt[\"Date\"].iloc[0].year\n",
        "\n",
        "    hiv4_pos = []\n",
        "    hiv4_neg = []\n",
        "    hiv4_tone = []\n",
        "\n",
        "    lm_pos = []\n",
        "    lm_neg = []\n",
        "    lm_tone = []\n",
        "\n",
        "    for index, row in df_year_splt.iterrows():\n",
        "      \n",
        "        tokens_hiv4 = hiv4.tokenize(row[\"Text\"])\n",
        "        tokens_lm = lm.tokenize(row[\"Text\"])\n",
        "\n",
        "        score_hiv4 = hiv4.get_score(tokens_hiv4)\n",
        "        score_lm = lm.get_score(tokens_lm)\n",
        "\n",
        "        hiv4_pos.append(score_hiv4[\"Positive\"])\n",
        "        hiv4_neg.append(score_hiv4[\"Negative\"])\n",
        "        hiv4_tone.append(score_hiv4[\"Positive\"] - score_hiv4[\"Negative\"])\n",
        "\n",
        "        lm_pos.append(score_lm[\"Positive\"])\n",
        "        lm_neg.append(score_lm[\"Negative\"])\n",
        "        lm_tone.append(score_lm[\"Positive\"] - score_lm[\"Negative\"])\n",
        "\n",
        "    data_fill.append([str(year), sum(hiv4_pos), sum(hiv4_neg), sum(hiv4_tone), sum(lm_pos), sum(lm_neg), sum(lm_tone)])\n",
        "\n",
        "df_by_year_dm = pd.DataFrame(data_fill ,columns = [\"Year\", \"HIV4 positive\", \"HIV4 negative\", \"HIV4 tone\", \"LM_pos\", \"LM_neg\", \"LM_tone\"])\n",
        "\n",
        "df_by_year_dm.loc[\"Total\"] = df_by_year_dm.sum(numeric_only=True, axis = 0)\n",
        "df_by_year_dm.at[\"Total\", \"Year\"] = \"All years\"\n",
        "\n",
        "print(df_by_year_dm)"
      ],
      "metadata": {
        "id": "RBxWA_49A10m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366d2ef1-7b30-4eff-b3f2-d888f8ad7d81"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Year  HIV4 positive  HIV4 negative  HIV4 tone     LM_pos     LM_neg   LM_tone\n",
            "0           2009         6181.0         3358.0     2823.0      942.0     1804.0    -862.0\n",
            "1           2010        75075.0        35830.0    39245.0    11625.0    16146.0   -4521.0\n",
            "2           2011       123012.0        56017.0    66995.0    18170.0    24594.0   -6424.0\n",
            "3           2012       222105.0        96681.0   125424.0    32302.0    37126.0   -4824.0\n",
            "4           2013       212709.0       103582.0   109127.0    31041.0    39428.0   -8387.0\n",
            "5           2014       468392.0       231155.0   237237.0    66679.0   108840.0  -42161.0\n",
            "6           2015       763503.0       372421.0   391082.0   116756.0   164112.0  -47356.0\n",
            "7           2016       968957.0       482229.0   486728.0   153482.0   182837.0  -29355.0\n",
            "8           2017      1508858.0       637895.0   870963.0   264627.0   255755.0    8872.0\n",
            "9           2018      4063978.0      1734882.0  2329096.0   740172.0   710158.0   30014.0\n",
            "10          2019      6414839.0      2613914.0  3800925.0  1207637.0  1017624.0  190013.0\n",
            "Total  All years     14827609.0      6367964.0  8459645.0  2643433.0  2558424.0   85009.0\n"
          ]
        }
      ]
    }
  ]
}