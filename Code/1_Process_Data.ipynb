{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Process data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing "
      ],
      "metadata": {
        "id": "zzA4vGQsYd-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code extracts the date, ticker and text from the json file in the NASDAQ data folder and creates a csv file with these three columns and an additional column displaying the word count of the respective text.\n",
        "\n",
        "Further the code adjusts the date format such that it can be used to fetch data from the wrds database.\n",
        "The text will be processed to reduce noise with the help of the functions from Frankel, Jennings and Lee (2021)"
      ],
      "metadata": {
        "id": "3gYWrci7Xqxb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5eAxlk6Xf_f"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import sys\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from nltk.stem.porter import *\n",
        "from datetime import datetime\n",
        "stemmer = PorterStemmer()\n",
        "exclude = set(string.punctuation)\n",
        "\n",
        "#To max out field limit\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "#### FUNCTIONS TO CLEAN PHRASES AND WORDS #### Frankel, Jennings and Lee (2021)\n",
        "\n",
        "stopwords = ['a','able','across','after','also','am','among','an','and','any','are','as','at','be','because','been','but','by','can','could','dear','did','do','does','either','else','ever','every','for','from','get','got','had','has','have','he','her','hers','him','his','how','however','i','if','in','into','is','it','its','just','let','like','likely','me','my','of','off','often','on','only','or','other','our','own','rather','said','say','says','she','should','since','so','some','than','that','the','their','them','then','there','these','they','this','tis','to','too','twas','us','wants','was','we','were','what','when','where','which','while','who','whom','why','will','with','would','yet','you','your']\n",
        "\n",
        "stopwords_dict={}\n",
        "for stopword in stopwords:\n",
        "    stopwords_dict[stopword]=0\n",
        "\n",
        "def fix_phrases(section):\n",
        "    section = re.sub('(\\d+)\\.(\\d+)','\\g<1>\\g<2>',section) # Remove periods from numbers -- 4.55 --> 455\n",
        "    section = section.replace(\".com\", \"com\")\n",
        "    section = section.replace(\"-\", \" \")\n",
        "    section = section.replace('. .', '.')\n",
        "    section = section.replace('.', 'XXYYZZ1')\n",
        "    section = ''.join(ch for ch in section if ch not in exclude) #Delete all punctuation except periods\n",
        "    section = section.replace('XXYYZZ1', '.')\n",
        "    section = section.lower()\n",
        "    section = re.sub(' +',' ',section) #Remove multiple spaces\n",
        "    if section == '.': section = ''\n",
        "    return section\n",
        "\n",
        "def fixword(word, portstem = True):\n",
        "        word = word.replace('\\n','')\n",
        "        if re.search('[0-9]',word) != None:\n",
        "            word = '00NUMBER00' # Replace numbers with 000NUMBER000\n",
        "        try:\n",
        "            test = stopwords_dict[word]\n",
        "            word = '_' # Replace stop words with _\n",
        "        except Exception:\n",
        "            donothing = 1\n",
        "        #Variable if stemming or not\n",
        "        if portstem:\n",
        "            try:\n",
        "                word = stemmer.stem(word)  # Stemp words\n",
        "            except Exception:\n",
        "                word = ''\n",
        "        return word\n",
        "\n",
        "#File paths\n",
        "json_file_path = \"/Users/alexanderholzer/PycharmProjects/Thesis/Data/Nasdaq/NASDAQ_News.json\"\n",
        "output_file_path = \"/Users/alexanderholzer/PycharmProjects/Thesis/Data/processed data/\"\n",
        "\n",
        "with open(json_file_path, 'r') as json_file:\n",
        "    with open(output_file_path + \"data_processed.csv\", \"w\") as csv_file:\n",
        "\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([\"Date\", \"Ticker\", \"Text\", \"Word_count\"])\n",
        "\n",
        "        for i, line in enumerate(json_file):\n",
        "\n",
        "            data = json.loads(line)\n",
        "\n",
        "            try:\n",
        "                date = data[\"article_time\"][\"$date\"]\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                ticker = data[\"symbols\"]\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                text = data[\"article_content\"]\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "            if len(ticker.split(sep=\",\")) == 1 and ticker != \"\":\n",
        "\n",
        "                print(ticker)\n",
        "\n",
        "                # Change date to proper format without T and Z\n",
        "                date = list(date)\n",
        "                date = [w.replace(\"T\", \" \") for w in date]\n",
        "                date = [w.replace(\"Z\", \" \") for w in date]\n",
        "                date = \"\".join(date)\n",
        "                date = pd.to_datetime(date.split(\" \")[0])\n",
        "\n",
        "                # Pre-processing text to reduce noise and prepare text data for word representation techniques (word embedding). Part of the code is taken from Frankel, Jennings and Lee (2021)\n",
        "                text = fix_phrases(text)\n",
        "                sentences = text.split('.')\n",
        "\n",
        "                count_words = 0\n",
        "\n",
        "                for v, sentence in enumerate(sentences):\n",
        "\n",
        "                    sentences[v] = sentences[v].replace(\".\", \"\").strip()\n",
        "\n",
        "                    allwords = sentences[v].split(\" \")\n",
        "\n",
        "                    for w, word in enumerate(allwords):\n",
        "                        count_words += 1\n",
        "                        allwords[w] = fixword(allwords[w])\n",
        "\n",
        "                    sentences[v] = \" \".join(allwords)\n",
        "\n",
        "                text = \".\".join(sentences)\n",
        "\n",
        "                writer.writerow([date.strftime(\"%Y-%m-%d\"), ticker, text, count_words])\n",
        "\n",
        "        json_file.close()\n",
        "\n",
        "\n",
        "txtticker_path = \"/Users/alexanderholzer/PycharmProjects/Thesis/Data/processed data/\"\n",
        "\n",
        "#Write txt file as file document supported on wrds database with txt ticker names to get permno to upload on wrds to get permnos as csv (select only latest permnos):https://wrds-www.wharton.upenn.edu/pages/get-data/center-research-security-prices-crsp/annual-update/tools/translate-to-permcopermno/\n",
        "with open(txtticker_path + \"dataticker.txt\", \"w\") as f:\n",
        "    for index, row in df.iterrows():\n",
        "          f.write(str(row[\"Ticker\"]) + \"\\n\")\n"
      ]
    }
  ]
}